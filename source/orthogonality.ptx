<section xml:id="section-orthogonality">
  <title>Orthogonality and Symmetry</title>
  <subsection xml:id="subsection-orthogonal-matrices">
    <title>Preserving Lengths and Angles</title>
    <definition>
      <statement>
        <p>
          An <m>n \times n</m> matrix <m>A</m> is called
          <term>orthogonal</term> if it preserves lengths. That is,
          for all <m>v \in \RR^n</m>, <m>|v| = |Av|</m>.
        </p>
      </statement>
    </definition>
    <p>
      Preservation of lengths is not the only way to define orthogonal
      matrices. The following proposition shows the rich structure of
      this group of matrices. Before stating the proposition, here is
      a useful definition.
    </p>
    <definition>
      <statement>
        <p>
          Let <m>i,j</m> be two indices which range from 1 to
          <m>n</m>. The <term>Kronecker delta</term> is a
          twice-indexed set of numbers <m>\delta_{ij}</m>, which
          evaluates to <m>0</m> whenever <m>i \neq j</m> and <m>1</m>
          whenever <m>i=j</m>.
          <me>
            \delta_{ij} = \begin{cases} 0 \amp i \neq j \\ 1
            \amp i = j \end{cases} 
          </me>
        </p>
      </statement>
    </definition>
    <p>
      The Kronecker delta is a convenient short-hand to indicate that
      matching indices are important. The expressions
      <m>\delta_{5,6}</m>, <m>\delta_{4,10}</m> and
      <m>\delta_{9,3}</m> all evaluate to <m>0</m> since the indices
      do not match.  The expressions <m>\delta_{4,4}</m>,
      <m>\delta_{17,17}</m> and <m>\delta_{1,1}</m> all evaluate to
      <m>1</m> since the indices do match. 
    </p>
    <proposition>
      <statement>
        <p>
          Let <m>A</m> be an <m>n \times n</m> matrix. Write <m>A =
          (a_1, a_2, \ldots, a_n)</m> where the <m>a_i</m> are column
          vectors (for each <m>a_i</m> is a vector in <m>\RR^n</m>).
          Then the following seven statements are all equivalent, and
          all can be taken as the definition of an orthogonal matrix.
          <ol>
            <li>
              <p>
                <m>A</m> preserves lengths:
                <m>|v| = |Av|</m> for all <m>v \in \RR^n</m>.
              </p>
            </li>
            <li>
              <p>
                <m>A</m> preserves dot products:
                <m>u\cdot v = (Au) \cdot (Av)</m> for all <m>u</m>,
                <m>v \in \RR^n</m>.
              </p>
            </li>
            <li>
              <p>
                <m>A^T = A^{-1}</m>.
              </p>
            </li>
            <li>
              <p>
                <m>A^TA = \Id_n</m>.
              </p>
            </li>
            <li>
              <p>
                <m>a_i \perp a_j</m> for all
                <m>i \neq j</m> and <m>|a_i| = 1</m>.
              </p>
            </li>
            <li>
              <p>
                <m>a_i \cdot a_j = \delta_{ij}</m>
              </p>
            </li>
            <li>
              <p>
                <m>A</m> preserves angles (the angle between <m>u</m>
                and <m>v</m> is the same as the angle between
                <m>Au</m> and <m>Av</m> for all <m>u</m>, <m>v \in
                \RR^n</m>) <em>and</em> <m>\det A = \pm 1</m>.
              </p>
            </li>
          </ol>
        </p>
      </statement>
    </proposition>
    <p>
      There are several very surprising statements here. First,
      preserving angles and preserving lengths are nearly equivalent
      (preserving angles needs the additional determinant condition).
      This is a little odd; there isn't a strong intuition that
      exactly the same transformations should preserve both
      properties. Second, there is a convenient algebraic method of
      identifying orthogonal matrices in property d): I can multiply
      by the transpose: if I get the identity, I know the matrix is 
      orthogonal. The equivalence of property d) and property f) is seen
      in simply calculating the matrix multiplication; it involves the
      dot products of all the columns, which must evaluate to <m>0</m>
      or <m>1</m> exactly matching the Kronecker delta.
    </p>
    <p>
      Orthogonal matrices can be thought of as rigid-body
      transformations. Since they preserve both lengths and angles,
      they preserve the shape of anything formed of vertices and
      lines: any polygon, polyhedron, or higher-dimensional analogue.
      They may not preserve the position (they may be moved around,
      rotated, reflected, etc.), but they will preserve the shape. This
      makes orthogonal matrices extremely useful since many
      applications want to use transformations that don't destroy
      polygons or polyhedra. Here is a proposition that gathers some
      other properties of orthogonal matrices. 
    </p>
    <proposition>
      <statement>
        <ul>
          <li>
            <p>
              In <m>\RR^2</m>, the only orthogonal transformations
              are the identity, the rotations and the reflections.
            </p>
          </li>
          <li>
            <p>
              If <m>A</m> is an orthogonal matrix, so is <m>A^{-1}</m>.
            </p>
          </li>
          <li>
            <p>
              All orthogonal matrices have determinant <m>\pm 1</m>.
            </p>
          </li>
          <li>
            <p>
              If <m>A</m> and <m>B</m> are orthogonal matrices,
              then so are <m>AB</m> and <m>BA</m>.
            </p>
          </li>
        </ul>
      </statement>
      <proof>
        <p>
          The proofs of these statements will be part of the activity
          for this week.  
        </p>
      </proof>
    </proposition>
    <example>
      <statement>
        <p>
          Even in <m>\RR^3</m>, orthogonality already gets a bit trickier
          than just the rotations/reflection of <m>\RR^2</m>. Consider
          the following matrix. 
          <me> 
            \begin{pmatrix} 
              -1 \amp  0 \amp  0 \\ 
              0 \amp  -1 \amp  0 \\ 
              0 \amp  0 \amp  -1 
            \end{pmatrix}
          </me>
        </p>
        <p>
          This matrix is orthogonal, but it isn't a rotation or
          reflection in <m>\RR^3</m>. It is some kind of <sq>reflection
          through the origin</sq> where every point is sent to the
          opposite point with respect to the origin. (The equivalent
          in <m>\RR^2</m> is a rotation by <m>\pi</m> radians). It isn't
          even a physical transformation: it's not something I can do
          with a physical object without destroying it. (If the object
          were, say, an inflatable beach-ball, this transformation
          would be equivalent to collasping the ball, turning it
          inside-out, and reinflating it that way). However, the
          transformations still satisfies the condition of
          orthogonality.
        </p>
      </statement>
    </example>
    <example>
      <statement>
        <p>
          Here is another example of an orthogonal matrix in
          <m>\RR^3</m>. 
          <me> 
            \begin{pmatrix} 
              \dfrac{5}{\sqrt{38}} \amp \dfrac{-3}{\sqrt{67}} 
                \dfrac{-23}{\sqrt{2546}} \\[1em]
              \dfrac{-3}{\sqrt{38}} \amp \dfrac{-7}{\sqrt{67}} 
                \dfrac{-9}{\sqrt{2546}} \\[1em]
              \dfrac{-2}{\sqrt{38}} \amp \dfrac{3}{\sqrt{67}} 
                \dfrac{-44}{\sqrt{2546}} \\
            \end{pmatrix}
          </me>
        </p>
        <p>
          To check the orthogonality, I would just take the dot
          products of the columns with each other. I should get
          <m>0</m> for each of these dot products to show that the
          columns are perpendicular to each other. Then I would check
          that the length of each column is <m>1</m>.
        </p>
      </statement>
    </example>
  </subsection>
</section>
